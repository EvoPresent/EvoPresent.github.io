<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="Deformable Neural Radiance Fields creates free-viewpoint portraits (nerfies) from casually captured videos.">
  <meta name="keywords" content="Nerfies, D-NeRF, NeRF">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>More Thinking, Less Seeing? Assessing Amplified Hallucination in Multimodal Reasoning Models</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/logo.svg">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>
    </div>
  </div>
</nav>


<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
         <img src="./static/figure/logo1.png" alt="Logo" style="height: 120px; margin-bottom: 20px;" />
            <h1 class="title is-1 publication-title" style="margin-top: 10px;">
            More Thinking, Less Seeing? Assessing Amplified Hallucination in Multimodal Reasoning Models
          </h1>
<!--           <h1 class="title is-1 publication-title">More Thinking, Less Seeing? Assessing Amplified Hallucination in Multimodal Reasoning Models</h1>
 -->
<!--   <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="">Chengzhi Liu*</a><sup>1</sup>1,3</span>
            <span class="author-block">
              <a href="">Zhongxing Xu*</a><sup>1</sup>3</span>
            <span class="author-block">
              <a href="">Qingyue Wei</a><sup>2</sup>,
            </span>
            <span class="author-block">
              <a href="">Juncheng Wu</a><sup>3</sup>,
            </span>
            <span class="author-block">
              <a href="">James Zoou</a><sup>2</sup>,
            </span>
            <span class="author-block">
              <a href="">Xin Eric Wang</a><sup>1,3</sup>,
            </span>
              </span>
            <span class="author-block">
              <a href="">Yuyin Zhou</a><sup>3</sup>,
            </span>
         <span class="author-block">
              <a href="">Sheng Liu</a><sup>2</sup>,
            </span>
          </div> -->

  <div class="is-size-5 publication-authors">
  <div>
    <span class="author-block">
      <a href="">Chengzhi Liu*</a><sup>1</sup>,
    </span>
    <span class="author-block">
      <a href="">Zhongxing Xu*</a><sup>1</sup>,
    </span>
    <span class="author-block">
      <a href="">Qingyue Wei</a><sup>2</sup>,
    </span>
    <span class="author-block">
      <a href="">Juncheng Wu</a><sup>3</sup>,
    </span>
  </div>
  <div>
    <span class="author-block">
      <a href="">James Zoou</a><sup>2</sup>,
    </span>
    <span class="author-block">
      <a href="">Xin Eric Wang</a><sup>1,3</sup>,
    </span>
    <span class="author-block">
      <a href="">Yuyin Zhou</a><sup>3</sup>,
    </span>
    <span class="author-block">
      <a href="">Sheng Liu</a><sup>2</sup>
    </span>
  </div>
    </div>

<div class="is-size-5 publication-authors" style="text-align: center;">
  <span><sup>1</sup>UC Santa Barbara, <sup>2</sup>Stanford University, <sup>1</sup>UC Santa Cruz</span>
</div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <span class="link-block">
                <a href="https://arxiv.org/abs/2410.06172"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>
              <!-- Code Link. -->
              <span class="link-block">
                <a href="https://github.com/eric-ai-lab/MSSBench"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span>
              <!-- Dataset Link. -->
              <span class="link-block">
                <a href="https://huggingface.co/datasets/kzhou35/mssbench/tree/main"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="far fa-images"></i>
                  </span>
                  <span>Data</span>
                  </a>
            </div>

          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">

    </div>
  </div>
</section>




<section class="section">
  <figure class="image is-centered" style="width: 65%; margin: 0 auto 1rem;  text-align: center;">
    <img 
      src="./static/figure/intro.png" 
      alt="image" 
      style="width: 100%; height: auto;" 
      loading="lazy" 
    />
    <figcaption style="text-align: center;">
      Figure 1. Illustration of multimodal situational safety. The model must judge the safety of the user's query or instruction based on the visual context and adjust their answer accordingly. Given an unsafe visual context, the model should remind the user of the potential risk instead of directly answering the user's query. However, current MLLMs struggle to achieve this in most unsafe situations.
    </figcaption>
  </figure>
</section>

  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
             Test-time compute has empowered multimodal large language models to generate extended reasoning chains, yielding strong performance on tasks such as multimodal math reasoning. However, we observe that this improved reasoning ability often comes with increased hallucination: as generations become longer, models tend to drift away from image-grounded content and rely more on language priors. Attention analysis reveals that longer reasoning chains reduce focus on visual inputs, contributing to hallucination. To systematically study this phenomenon, we introduce <em>RH-AUC</em>, a metric that quantifies how a model's perception accuracy changes with reasoning length, enabling evaluation of whether the model preserves visual grounding while reasoning. We also release <em>RH-Bench</em>, a diagnostic benchmark covering diverse multimodal tasks, designed to jointly assess the balance of reasoning ability and hallucination. We find that <em>(i)</em> larger models generally exhibit a better balance between reasoning and perception; <em>(ii)</em> reasoning and perception balance depends more on the types and domains of the training data than its volume. Our findings highlight the need for evaluation frameworks that account for both reasoning quality and perceptual reliability.

          </p>
        </div>
      </div>
    </div>
 
<!-- </section> -->
       <div class="columns is-centered has-text-centered" style="margin-top: 2rem;">
      <div class="column is-four-fifths">
      <h2 class="title is-3">Multimodal Reasoning Can Amplify Visual Hallucination</h2>
      </div>
      </div>
        
 <section class="section">
  <div class="columns is-centered" style="margin-bottom: 0.25rem;">
    <!-- 左侧图片 -->
    <div class="column is-half" style="text-align: center;">
      <figure class="image">
        <img 
          src="./static/category.png" 
          alt="image" 
          style="width: 100%; height: auto;"  
          loading="lazy"
        />
        <figcaption style="text-align: center;">
          Figure 2. Presentation of MSSBench across four domains and ten secondary categories in Chat and Embodied tasks.
      </figure>
    </div>
    
    <!-- 右侧图片 -->
    <div class="column is-half" style="text-align: center;">
      <figure class="image">
        <img 
          src="./static/static.png" 
          alt="image" 
          style="width: 100%; height: auto;"  
          loading="lazy"
        />
        <figcaption style="text-align: center;">
          Figure 3. Data statistics for multimodal situational safety categories with percentages.
        </figcaption>
      </figure>
    </div>
  </div>
</section>



<div class="columns is-centered has-text-centered" style="margin-top: 0rem;">
  <div class="column is-four-fifths">
    <h2 class="title is-3">Why Reasoning Models Amplify Hallucinations?</h2>
  </div>
</div>
    
<section class="section">
  <div class="columns is-centered" style="margin-bottom: 0rem;">
    <figure class="image" style="width: 100%;">
      <div class="columns">
        <!-- 左侧图片占 68% 宽度 -->
        <div class="column is-8" style="text-align: center;">
          <img 
            src="./static/pipline_overall.png" 
            alt="image" 
            style="width: 100%; height: auto;" 
            loading="lazy"
          />
        </div>
        <!-- 右侧图片占 32% 宽度 -->
        <div class="column is-4" style="text-align: center;">
          <img 
            src="./static/pipline_example.png" 
            alt="image" 
            style="width: 100%; height: auto;" 
            loading="lazy"
          />
        </div>
      </div>
      <!-- 总的figcaption -->
      <figcaption style="text-align: center;">
        Figure 4. The overall structure of the chat data collection pipeline (left) and examples of two multimodal assistant scenarios (right). The pipeline includes four parts: (1) Generating Intented Activity and Unsafe Textual Situations. (2) Iterative Filtering with LLM. (3) Constructing a Multimodal Situational Safety Dataset via Image Retrieval. (4) Human Verification \& Query Generation. 
      </figcaption>
    </figure>
  </div>
</section>

    
<div class="columns is-centered has-text-centered" style="margin-top: -0.5rem;">
      <div class="column is-four-fifths">
      <h2 class="title is-3">Effects of Reasoning Length on Reasoning-Hallucination Balance</h2>
      </p>
        We assess the performance of 8 leading multimodal large language models (MLLMs) on our MSS benchmark.
      </p>
      </div>
      </div>
    
<section class="section" style="margin-top: -2.6rem;">
  <figure class="image is-centered" style="width: 80%; margin: 0 auto 1rem; text-align: center;">
    <img 
      src="./static/table1.png" 
      alt="image" 
      style="width: 100%; height: auto;" 
      loading="lazy"
    />
    <figcaption style="text-align: center;">
      Table 1. Accuracy of MLLMs under instruction following setting. All of the MLLMs struggle to respond with safety awareness under unsafe situations and perform even worse in Embodied Task.
    </figcaption>
  </figure>
</section>

<section class="section" style="margin-top: -1.5rem;">
  <div class="content has-text-centered">
    <p>
   We identify three main reasons for MLLM's poor performance on the MSS benchmark: lack of explicit safety reasoning, visual understanding, and situational safety judgment. To validate these hypotheses, we design four distinct evaluation settings: (1) explicit safety reasoning for user queries, (2) explicit safety reasoning for user intents, (3) explicit safety reasoning for user intents with self-captioning, and (4) explicit safety reasoning for user intents using ground-truth situation information. 
    </p>
  </div>
</section>


<section class="section">
  <figure>
    <!-- 第一排 -->
    <div class="columns is-centered" style="margin-top: -2.5rem;">
      <div class="column is-4" style="text-align: center;">
        <figure class="image">
          <img 
            src="./static/chat_safe.png" 
            alt="image" 
            style="width: 100%; height: auto;" 
            loading="lazy" 
          />
          <figcaption style="text-align: center;">
            Chat task safe situations.
          </figcaption>
        </figure>
      </div>
      <div class="column is-4" style="text-align: center;">
        <figure class="image">
          <img 
            src="./static/chat_unsafe.png" 
            alt="image" 
            style="width: 100%; height: auto;" 
            loading="lazy" 
          />
          <figcaption style="text-align: center;">
            Chat task unsafe situations.
          </figcaption>
        </figure>
      </div>
      <div class="column is-4" style="text-align: center;">
        <figure class="image">
          <img 
            src="./static/chat_avg.png" 
            alt="image" 
            style="width: 100%; height: auto;" 
            loading="lazy" 
          />
          <figcaption style="text-align: center;">
            Chat task average.
          </figcaption>
        </figure>
      </div>
    </div>

    <!-- 第二排 -->
    <div class="columns is-centered" style="margin-bottom: 2rem;">
      <div class="column is-4" style="text-align: center;">
        <figure class="image">
          <img 
            src="./static/e_safe.png" 
            alt="image" 
            style="width: 100%; height: auto;" 
            loading="lazy" 
          />
          <figcaption style="text-align: center;">
             Embodied task safe situations.
          </figcaption>
        </figure>
      </div>
      <div class="column is-4" style="text-align: center;">
        <figure class="image">
          <img 
            src="./static/e_unsafe.png" 
            alt="image" 
            style="width: 100%; height: auto;" 
            loading="lazy" 
          />
          <figcaption style="text-align: center;">
          Embodied task unsafe situations.
          </figcaption>
        </figure>
      </div>
      <div class="column is-4" style="text-align: center;">
        <figure class="image">
          <img 
            src="./static/e_avg.png" 
            alt="image" 
            style="width: 100%; height: auto;" 
            loading="lazy" 
          />
          <figcaption style="text-align: center;">
           Embodied task average.
          </figcaption>
        </figure>
      </div>
    </div>
    <!-- 总的figcaption -->
  <figcaption style="text-align: center;">
   Table 2. Diagnosis of different factors influencing the MLLM’s situational safety performance.
  Besides the <strong>instruction following (IF)</strong> setting, we design four extra settings: 
  (1) <strong>query classification (QC)</strong>: letting MLLMs explicitly reason the safety of user query, 
  (2) <strong>intent classification (IC)</strong>: explicitly reason the safety of user’s intent, 
  (3) <strong>IC w/ Self Cap</strong>: explicitly reason the safety of user’s intent providing with self-caption, and 
  (4) <strong>IC w/ GT Cap</strong>: explicitly reason the safety of user’s intent providing with ground-truth situation information. 
  We report and compare the individual (a) and average (b) performance of open-source MLLMs and closed-source MLLMs.
</figcaption>
  </figure>
</section>



<div class="columns is-centered has-text-centered" style="margin-top: 1rem;">
      <div class="column is-four-fifths">
      <h2 class="title is-3">Evaluation on the Reasoning-Hallucination Balance</h2>
      </div>
      </div>
    
<section class="section">
  <figure class="image is-centered" style="width: 100%; margin: 0 auto 2rem; text-align: center;">
    <img 
      src="./static/agent.png" 
      alt="image" 
      style="width: 100%; height: auto;" 
      loading="lazy" 
    />
    <figcaption style="text-align: center;">
      Figure 5. Workflow of our Multi-Agent framework for enhancing situational safety in user queries, incorporating Intent Reasoning, Safety Judgment, QA and Visual Understanding agents.
    </figcaption>
  </figure>
</section>


<!-- 固定布局的图片展示，不使用响应式 -->
<section class="section">
  <div class="columns"style="margin-top: -0.6rem;">
    <!-- 左侧图片 -->
    <div class="column" style="text-align: center; width: 50%;">
      <figure class="image" style="margin: 0 auto;">
        <img 
          src="./static/agent_chat.png" 
          alt="image" 
          style="width: 600px; height: auto;" 
          loading="lazy"
        />
      </figure>
    </div>
    
    <div class="column" style="text-align: center; width: 50%;">
      <figure class="image" style="margin: 0 auto;">
        <img 
          src="./static/agent_embodied.png" 
          alt="image" 
          style="width: 600px; height: auto;" 
          loading="lazy"
        />
      </figure>
    </div>
  </div>
  <figcaption style="text-align: center; margin-top: 0.5rem;">
    Table 3. MLLM's performance on our benchmark with three reasoning settings. Base setting: without explicit safety reasoning. 1 step CoT: MLLMs reasoning the safety of user query and generating response at one step. Multi-agent: our designed multi-agent pipeline. The results show that the multi-agent pipeline improves performance in most cases.
  </figcaption>
</section>

<!-- 固定布局的 BibTeX 部分 -->
<section class="section" id="BibTeX">
  <div class="container content" style="max-width: 800px; margin: 0 auto;">
    <h2 class="title">BibTeX</h2>
    <pre><code>@misc{zhou2024multimodalsituationalsafety,
      title={Multimodal Situational Safety}, 
      author={Kaiwen Zhou and Chengzhi Liu and Xuandong Zhao and Anderson Compalas and Dawn Song and Xin Eric Wang},
      year={2024},
      eprint={2410.06172},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/2410.06172}, 
}</code></pre>
  </div>
</section>


